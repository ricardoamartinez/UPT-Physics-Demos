# Universal Physics Transformer

[[`Project Page`](https://ml-jku.github.io/UPT)] [[`Paper (arxiv)`](https://arxiv.org/abs/2402.12365)] [[`Codebase Demo Video`](https://youtu.be/80kc3hscTTg)] [[`BibTeX`](https://github.com/ml-jku/UPT#citation)]

# Train your own models

Instructions to setup the codebase on your own environment are provided in
[SETUP_CODE](https://github.com/ml-jku/UPT/blob/main/SETUP_CODE.md),
[SETUP_DATA](https://github.com/ml-jku/UPT/blob/main/SETUP_DATA.md).

A video to motivate design choices of the codebase and give an overview of the codebase can be
found [here](https://youtu.be/80kc3hscTTg).

Configurations to train models can be found [here](https://github.com/ml-jku/UPT/tree/main/src/yamls).

# Citation

If you like our work, please consider giving it a star :star: and cite us

```
@article{alkin2024upt,
      title={Universal Physics Transformers}, 
      author={Benedikt Alkin and Andreas FÃ¼rst and Simon Schmid and Lukas Gruber and Markus Holzleitner and Johannes Brandstetter},
      journal={arXiv preprint arXiv:2402.12365},
      year={2024}
}
```
